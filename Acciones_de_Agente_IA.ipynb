{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Antonio14-code/agenteIA/blob/main/Acciones_de_Agente_IA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install --upgrade --user google-cloud-aiplatform"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oKR93ViR9kT",
        "outputId": "a5deb2ec-cee5-472b-878b-2bee157ba77c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-aiplatform in /root/.local/lib/python3.11/site-packages (1.85.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (2.24.2)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (5.29.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (24.2)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0,>=1.32.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (2.19.0)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (3.29.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0,>=1.3.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (1.14.2)\n",
            "Requirement already satisfied: shapely<3.0.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (2.0.7)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (2.10.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (4.12.2)\n",
            "Requirement already satisfied: docstring-parser<1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (0.16)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.69.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (4.9)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.7.2)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-resource-manager<3.0.0,>=1.3.3->google-cloud-aiplatform) (0.14.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage<3.0.0,>=1.32.0->google-cloud-aiplatform) (1.7.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->google-cloud-aiplatform) (2.27.2)\n",
            "Requirement already satisfied: numpy<3,>=1.14 in /usr/local/lib/python3.11/dist-packages (from shapely<3.0.0->google-cloud-aiplatform) (2.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0dev,>=2.7.3->google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwpLVZ_CSAix",
        "outputId": "5a903fd1-5a3e-4eec-be37-70291074ccdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "# Additional authentication is required for Google Colab\n",
        "if \"google.colab\" in sys.modules:\n",
        "    # Authenticate user to Google Cloud\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ],
      "metadata": {
        "id": "mDypWxKLSDok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import vertexai\n",
        "\n",
        "PROJECT_ID = \"my-proyecto-conauti-01\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ],
      "metadata": {
        "id": "iLC8sbVuSGVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import sys\n",
        "import traceback\n",
        "from typing import Callable, Tuple\n",
        "\n",
        "from google.protobuf.json_format import MessageToJson\n",
        "\n",
        "from vertexai import generative_models\n",
        "from vertexai.generative_models import (\n",
        "    FunctionDeclaration, #Function Calling\n",
        "    GenerativeModel,\n",
        "    GenerationConfig,\n",
        "    Part, #Function Calling\n",
        "    Tool, #Function Calling\n",
        ")\n",
        "\n",
        "#Configurar el modo de funcionamiento de Function Calling\n",
        "from vertexai.preview.generative_models import (\n",
        "    ToolConfig #Function Calling\n",
        ")"
      ],
      "metadata": {
        "id": "v4XrvIHlSKcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Configurar Nuestro Modelo\n",
        "\n",
        "#Seguridad\n",
        "Configuracion_Seguridad = {\n",
        "    vertexai.preview.generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: vertexai.preview.generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
        "    vertexai.preview.generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: vertexai.preview.generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
        "    vertexai.preview.generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: vertexai.preview.generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
        "    vertexai.preview.generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: vertexai.preview.generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
        "}\n",
        "\n",
        "#Configuración\n",
        "configuracion_Modelo = {\n",
        "    \"max_output_tokens\": 8192,\n",
        "    \"temperature\":0.2,\n",
        "    #\"top_k\":10,\n",
        "    \"top_p\":0.95\n",
        "}"
      ],
      "metadata": {
        "id": "yug-wFftSOsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GenerativeModel(\n",
        "    \"gemini-1.5-pro-001\",\n",
        "    system_instruction=[\n",
        "        \"You are an assistant that helps me tidy my room.\"\n",
        "        \"Your goal is to make sure all the books are on the shelf, all clothes are in the hamper, and the trash is empty.\",\n",
        "        \"You cannot receive any input from me.\",\n",
        "    ],\n",
        "    generation_config=configuracion_Modelo,\n",
        "    safety_settings=Configuracion_Seguridad,\n",
        "#    safety_settings=[\n",
        "#        generative_models.SafetySetting(\n",
        "#            category=generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
        "#            method=generative_models.SafetySetting.HarmBlockMethod.PROBABILITY,\n",
        "#            threshold=generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
        "#        ),\n",
        "#],\n",
        ")"
      ],
      "metadata": {
        "id": "0TbDpwTlSSde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "verbose = True"
      ],
      "metadata": {
        "id": "aUWdFMwhSV5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conveience function to print multiline text indented\n",
        "def indent(text, amount, ch=\" \"):\n",
        "    padding = amount * ch\n",
        "    return \"\".join(padding + line for line in text.splitlines(True))\n",
        "\n",
        "\n",
        "# Convenience function for logging statements\n",
        "def logging(msg):\n",
        "    global verbose\n",
        "    print(msg) if verbose else None\n",
        "\n",
        "\n",
        "# Retrieve the text from a model response\n",
        "def get_text(resp):\n",
        "    return resp.candidates[0].content.parts[0].text\n",
        "\n",
        "\n",
        "# Retrieve the function call information from a model response\n",
        "def get_function_call(resp):\n",
        "    return resp.candidates[0].function_calls[0]\n",
        "\n",
        "\n",
        "def get_action_label(json_payload, log, role=\"MODEL\"):\n",
        "    log(f\"{role}: {json_payload}\")\n",
        "    answer = json.loads(json_payload)\n",
        "    action = answer[\"next_action\"]\n",
        "    return action\n",
        "\n",
        "\n",
        "def get_action_from_function_call(func_payload, log, role=\"MODEL\"):\n",
        "    json_payload = MessageToJson(func_payload._pb)\n",
        "    log(f\"{role}: {json_payload}\")\n",
        "    return func_payload.name"
      ],
      "metadata": {
        "id": "A7vy4GzCSYko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial room state\n",
        "def reset_room_state(room_state):\n",
        "    room_state.clear()\n",
        "    room_state[\"clothes\"] = \"floor\"\n",
        "    room_state[\"books\"] = \"scattered\"\n",
        "    room_state[\"wastebin\"] = \"empty\"\n",
        "\n",
        "\n",
        "# Functions for actions (replace these with Gemini function calls)\n",
        "def pick_up_clothes(room_state):\n",
        "    room_state[\"clothes\"] = \"carrying by hand\"\n",
        "    return room_state, \"The clothes are now being carried.\"\n",
        "\n",
        "\n",
        "def put_clothes_in_hamper(room_state):\n",
        "    room_state[\"clothes\"] = \"hamper\"\n",
        "    return room_state, \"The clothes are now in the hamper.\"\n",
        "\n",
        "\n",
        "def pick_up_books(room_state):\n",
        "    room_state[\"books\"] = \"in hand\"\n",
        "    return room_state, \"The books are now in my hand.\"\n",
        "\n",
        "\n",
        "def place_books_on_shelf(room_state):\n",
        "    room_state[\"books\"] = \"shelf\"\n",
        "    return room_state, \"The books are now on the shelf.\"\n",
        "\n",
        "\n",
        "def empty_wastebin(room_state):\n",
        "    room_state[\"wastebin\"] = \"empty\"\n",
        "    return room_state, \"The wastebin is emptied.\"\n",
        "\n",
        "\n",
        "# Maps a function string to its respective function reference.\n",
        "def get_func(action_label):\n",
        "    return None if action_label == \"\" else getattr(sys.modules[__name__], action_label)"
      ],
      "metadata": {
        "id": "t21wZY6QSbBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to check if the room is tidy\n",
        "# Some examples below do not call this function,\n",
        "# for those examples the model takes on the goal validation role.\n",
        "def is_room_tidy(room_state):\n",
        "    return all(\n",
        "        [\n",
        "            room_state[\"clothes\"] == \"hamper\",\n",
        "            room_state[\"books\"] == \"shelf\",\n",
        "            room_state[\"wastebin\"] == \"empty\",\n",
        "        ]\n",
        "    )"
      ],
      "metadata": {
        "id": "5e2exMthSedh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "functions = \"\"\"\n",
        "<actions>\n",
        "    put_clothes_in_hamper - place clothes into hamper, instead of carrying them around in your hand.\n",
        "    pick_up_clothes - pick clothes up from the floor.\n",
        "    pick_up_books - pick books up from anywhere not on the shelf\n",
        "    place_books_on_shelf - self explanatory.\n",
        "    empty_wastebin - self explanatory.\n",
        "    done - when everything are in the right place.\n",
        "</actions>\"\"\"\n",
        "\n",
        "\n",
        "def get_next_step_full_prompt(state, cycle, log):\n",
        "    observation = f\"The room is currently in this state: {state}.\"\n",
        "    prompt = \"\\n\".join(\n",
        "        [\n",
        "            observation,\n",
        "            f\"You can pick any of the following action labels: {functions}\",\n",
        "            \"Which one should be the next step to achieve the goal? \",\n",
        "            'Return a single JSON object containing fields \"next_action\" and \"rationale\".',\n",
        "        ]\n",
        "    )\n",
        "    log(\"PROMPT:\\n{}\".format(indent(prompt, 1, \"\\t\"))) if cycle == 1 else log(\n",
        "        f\"OBSERVATION: {observation}\"\n",
        "    )\n",
        "\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "9GzBHUWPSiae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main ReAct loop\n",
        "def main_react_loop(loop_continues, log):\n",
        "    room_state = {}\n",
        "    reset_room_state(room_state)\n",
        "    trash_added = False\n",
        "\n",
        "    cycle = 1\n",
        "    while loop_continues(cycle, room_state):\n",
        "        log(f\"Cycle #{cycle}\")\n",
        "\n",
        "        # Observe the environment (use Gemini to generate an action thought)\n",
        "        try:  # REASON #\n",
        "            response = model.generate_content(\n",
        "                get_next_step_full_prompt(room_state, cycle, log),\n",
        "                generation_config={\"response_mime_type\": \"application/json\"},\n",
        "            )  # JSON Mode\n",
        "            action_label = get_action_label(get_text(response).strip(), log)\n",
        "\n",
        "        except Exception:\n",
        "            traceback.print_exc()\n",
        "            log(response)\n",
        "            break\n",
        "\n",
        "        # Execute the action and get the observation\n",
        "        if action_label == \"done\":\n",
        "            break\n",
        "\n",
        "        try:  # ACTION #\n",
        "            # Call the function mapped from the label\n",
        "            room_state, acknowledgement = get_func(action_label)(room_state)\n",
        "            log(f\"ACTION:   {action_label}\\nEXECUTED: {acknowledgement}\\n\")\n",
        "\n",
        "        except Exception:\n",
        "            log(\"No action suggested.\")\n",
        "\n",
        "        # Simulating a change in environment\n",
        "        if cycle == 4 and not trash_added:\n",
        "            room_state[\"wastebin\"] = \"1 item\"\n",
        "            trash_added = True\n",
        "\n",
        "        cycle += 1\n",
        "        # End of while loop\n",
        "\n",
        "    # Determine the final result\n",
        "    result = (\n",
        "        \"The room is tidy!\" if is_room_tidy(room_state) else \"The room is not tidy!\"\n",
        "    )\n",
        "\n",
        "    return room_state, result"
      ],
      "metadata": {
        "id": "tWL2l2CGSpr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#como inicializa el valor del input 1 para la función main_react loop\n",
        "room_state={}\n",
        "#print(room_state)\n",
        "reset_room_state(room_state)\n",
        "print(f'El cuarto empieza en el estado -> {room_state}')\n",
        "is_room_tidy(room_state)\n",
        "c=1\n",
        "print(f'El ciclo de trabajo del Agente IA empieza en: {c}')\n",
        "r=room_state\n",
        "resultado=lambda c, r: c <= 10 and not is_room_tidy(r)\n",
        "print(f'Vamos a inicializar la funcion loop main_react_loop con : {resultado(c,r)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01g83pnySzhk",
        "outputId": "e39fcc17-94ff-44fb-cd10-bf663602192f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El cuarto empieza en el estado -> {'clothes': 'floor', 'books': 'scattered', 'wastebin': 'empty'}\n",
            "El ciclo de trabajo del Agente IA empieza en: 1\n",
            "Vamos a inicializar la funcion loop main_react_loop con : True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#como inicializa el valor del input 2 para la función main_react loop\n",
        "print(logging)"
      ],
      "metadata": {
        "id": "OZYIPSaxS17X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10ecff66-4f0c-46d2-d660-e070b5cd5dec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<function logging at 0x7de9843979c0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We are passing in a while loop continuation test function:\n",
        "# Continue while loop when number of cycles <= 10 AND the room is not yet tidy.\n",
        "# We are explicitly testing if the room is tidy within code.\n",
        "#\n",
        "# To save space, only the first cycle prints the full prompt.\n",
        "# The same prompt template is used for every model call with a modified room state.\n",
        "room_state, result = main_react_loop(\n",
        "    lambda c, r: c <= 10 and not is_room_tidy(r), logging\n",
        ")\n",
        "print(room_state, result)"
      ],
      "metadata": {
        "id": "1S_VEi_3S5gH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e3a7a6a-656b-428e-facb-74a1e38d1410"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cycle #1\n",
            "PROMPT:\n",
            "\tThe room is currently in this state: {'clothes': 'floor', 'books': 'scattered', 'wastebin': 'empty'}.\n",
            "\tYou can pick any of the following action labels: \n",
            "\t<actions>\n",
            "\t    put_clothes_in_hamper - place clothes into hamper, instead of carrying them around in your hand.\n",
            "\t    pick_up_clothes - pick clothes up from the floor.\n",
            "\t    pick_up_books - pick books up from anywhere not on the shelf\n",
            "\t    place_books_on_shelf - self explanatory.\n",
            "\t    empty_wastebin - self explanatory.\n",
            "\t    done - when everything are in the right place.\n",
            "\t</actions>\n",
            "\tWhich one should be the next step to achieve the goal? \n",
            "\tReturn a single JSON object containing fields \"next_action\" and \"rationale\".\n",
            "MODEL: {\"next_action\": \"pick_up_clothes\", \"rationale\": \"The clothes are on the floor and need to be picked up before they can be put in the hamper.\"}\n",
            "ACTION:   pick_up_clothes\n",
            "EXECUTED: The clothes are now being carried.\n",
            "\n",
            "Cycle #2\n",
            "OBSERVATION: The room is currently in this state: {'clothes': 'carrying by hand', 'books': 'scattered', 'wastebin': 'empty'}.\n",
            "MODEL: {\"next_action\": \"put_clothes_in_hamper\", \"rationale\": \"The clothes are currently being carried.  Placing them in the hamper is the most logical next step.\"}\n",
            "ACTION:   put_clothes_in_hamper\n",
            "EXECUTED: The clothes are now in the hamper.\n",
            "\n",
            "Cycle #3\n",
            "OBSERVATION: The room is currently in this state: {'clothes': 'hamper', 'books': 'scattered', 'wastebin': 'empty'}.\n",
            "MODEL: {\"next_action\": \"pick_up_books\", \"rationale\": \"The books need to be on the shelf, so first we need to pick them up.\"}\n",
            "ACTION:   pick_up_books\n",
            "EXECUTED: The books are now in my hand.\n",
            "\n",
            "Cycle #4\n",
            "OBSERVATION: The room is currently in this state: {'clothes': 'hamper', 'books': 'in hand', 'wastebin': 'empty'}.\n",
            "MODEL: {\"next_action\": \"place_books_on_shelf\", \"rationale\": \"The books need to be on the shelf, and they are currently in hand.\"}\n",
            "ACTION:   place_books_on_shelf\n",
            "EXECUTED: The books are now on the shelf.\n",
            "\n",
            "Cycle #5\n",
            "OBSERVATION: The room is currently in this state: {'clothes': 'hamper', 'books': 'shelf', 'wastebin': '1 item'}.\n",
            "MODEL: {\"next_action\": \"empty_wastebin\", \"rationale\": \"The wastebin has one item in it and needs to be emptied to achieve the goal.\"}\n",
            "ACTION:   empty_wastebin\n",
            "EXECUTED: The wastebin is emptied.\n",
            "\n",
            "{'clothes': 'hamper', 'books': 'shelf', 'wastebin': 'empty'} The room is tidy!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We are passing in a while loop continuation test function:\n",
        "# Continue while loop when number of cycles <= 10\n",
        "# We are no longer testing if the room is tidy within code.\n",
        "# The decision is now up to the model.\n",
        "room_state, result = main_react_loop(lambda c, r: c <= 10, logging)\n",
        "print(room_state, result)"
      ],
      "metadata": {
        "id": "QSf_lzW0S-EQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2366963-f40a-4a0a-82f0-7fb3e1c4d56f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cycle #1\n",
            "PROMPT:\n",
            "\tThe room is currently in this state: {'clothes': 'floor', 'books': 'scattered', 'wastebin': 'empty'}.\n",
            "\tYou can pick any of the following action labels: \n",
            "\t<actions>\n",
            "\t    put_clothes_in_hamper - place clothes into hamper, instead of carrying them around in your hand.\n",
            "\t    pick_up_clothes - pick clothes up from the floor.\n",
            "\t    pick_up_books - pick books up from anywhere not on the shelf\n",
            "\t    place_books_on_shelf - self explanatory.\n",
            "\t    empty_wastebin - self explanatory.\n",
            "\t    done - when everything are in the right place.\n",
            "\t</actions>\n",
            "\tWhich one should be the next step to achieve the goal? \n",
            "\tReturn a single JSON object containing fields \"next_action\" and \"rationale\".\n",
            "MODEL: {\"next_action\": \"pick_up_clothes\", \"rationale\": \"The clothes are on the floor, so we need to pick them up first before we can put them in the hamper.\"}\n",
            "ACTION:   pick_up_clothes\n",
            "EXECUTED: The clothes are now being carried.\n",
            "\n",
            "Cycle #2\n",
            "OBSERVATION: The room is currently in this state: {'clothes': 'carrying by hand', 'books': 'scattered', 'wastebin': 'empty'}.\n",
            "MODEL: {\"next_action\": \"put_clothes_in_hamper\", \"rationale\": \"The clothes need to be in the hamper, and they are currently being carried. So the next step is to put them in the hamper.\"}\n",
            "ACTION:   put_clothes_in_hamper\n",
            "EXECUTED: The clothes are now in the hamper.\n",
            "\n",
            "Cycle #3\n",
            "OBSERVATION: The room is currently in this state: {'clothes': 'hamper', 'books': 'scattered', 'wastebin': 'empty'}.\n",
            "MODEL: {\"next_action\": \"pick_up_books\", \"rationale\": \"The goal is to have all books on the shelf, so we need to pick them up first.\"}\n",
            "ACTION:   pick_up_books\n",
            "EXECUTED: The books are now in my hand.\n",
            "\n",
            "Cycle #4\n",
            "OBSERVATION: The room is currently in this state: {'clothes': 'hamper', 'books': 'in hand', 'wastebin': 'empty'}.\n",
            "MODEL: {\"next_action\": \"place_books_on_shelf\", \"rationale\": \"The books need to be on the shelf, and you are holding them.\"}\n",
            "ACTION:   place_books_on_shelf\n",
            "EXECUTED: The books are now on the shelf.\n",
            "\n",
            "Cycle #5\n",
            "OBSERVATION: The room is currently in this state: {'clothes': 'hamper', 'books': 'shelf', 'wastebin': '1 item'}.\n",
            "MODEL: {\"next_action\": \"empty_wastebin\", \"rationale\": \"The wastebin has one item in it and needs to be emptied.\"}\n",
            "ACTION:   empty_wastebin\n",
            "EXECUTED: The wastebin is emptied.\n",
            "\n",
            "Cycle #6\n",
            "OBSERVATION: The room is currently in this state: {'clothes': 'hamper', 'books': 'shelf', 'wastebin': 'empty'}.\n",
            "MODEL: {\"next_action\": \"done\", \"rationale\": \"The clothes are already in the hamper, the books are on the shelf, and the wastebin is empty.\"}\n",
            "{'clothes': 'hamper', 'books': 'shelf', 'wastebin': 'empty'} The room is tidy!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#usamos Gemini\n",
        "from vertexai.generative_models import (\n",
        "    GenerationConfig,\n",
        "    GenerativeModel,\n",
        "    Part,\n",
        ")"
      ],
      "metadata": {
        "id": "DiE9_HNNW7-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TextGenerationModel.from_pretrained(\"text-bison@001\") Descontinuado\n",
        "multimodal_model = GenerativeModel(\"gemini-1.5-pro-001\")"
      ],
      "metadata": {
        "id": "ey1Ui0q-W2Bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config = GenerationConfig(\n",
        "    temperature=1, #0 - 2\n",
        "    top_p=0.5,\n",
        "    #top_k=32, # 1.5 PRO no existe\n",
        "    candidate_count=1,\n",
        "    max_output_tokens=4000, #max 8192 tokens\n",
        ")"
      ],
      "metadata": {
        "id": "3UndzKHVXF-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "room_state = '{\"ropa\": \"desordenada\", \"libros\": \"en el suelo\", \"juguetes\": \"esparcidos\"}'\n",
        "result = '{\"estado_general\": \"desordenado\"}'\n",
        "\n",
        "room_state_json = json.loads(room_state)\n",
        "result_json = json.loads(result)\n",
        "\n",
        "print(\"room_state_dict:\", room_state_json)\n",
        "print(\"result_dict:\", result_json)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtVdC7gB87RN",
        "outputId": "4220198f-21a1-4acc-9677-d0eae1cb7e1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "room_state_dict: {'ropa': 'desordenada', 'libros': 'en el suelo', 'juguetes': 'esparcidos'}\n",
            "result_dict: {'estado_general': 'desordenado'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "functions = \"\"\"\n",
        "<acciones>\n",
        "    poner_ropa_en_cesto - colocar la ropa en el cesto, en lugar de llevarla en la mano.\n",
        "    recoger_ropa - recoger la ropa del suelo.\n",
        "    recoger_libros - recoger los libros de cualquier lugar que no sea la estantería.\n",
        "    colocar_libros_en_estanteria - autoexplicativo.\n",
        "    vaciar_papelera - autoexplicativo.\n",
        "    listo - cuando todo esté en su lugar correcto.\n",
        "</acciones>\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Estado de la habitación: {room_state_json}\n",
        "Resultado del análisis: {result_json}\n",
        "\n",
        "Considerando el estado actual de la habitación, ¿cuál sería el orden más eficiente para organizarla?\n",
        "Utiliza las siguientes acciones para describir los pasos detallados:\n",
        "\n",
        "{functions}\n",
        "\n",
        "Proporciona la respuesta en formato de una lista numerada, usando las acciones definidas.\n",
        "\"\"\"\n",
        "\n",
        "contents = [\n",
        "    prompt,\n",
        "]\n",
        "\n",
        "\n",
        "responses = multimodal_model.generate_content(contents, generation_config=generation_config, stream=True)\n",
        "\n",
        "print(\"\\n-------Respuesta--------\")\n",
        "for response in responses:\n",
        "    print(response.text, end=\"\")"
      ],
      "metadata": {
        "id": "jrS6FR2OWodQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d05ee778-25b5-46c2-cde8-be7b4a1b1013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-------Respuesta--------\n",
            "Aquí tienes un posible orden eficiente para organizar la habitación, considerando su estado actual:\n",
            "\n",
            "1. **`recoger_ropa`**: Es mejor empezar por la ropa, ya que está \"desordenada\" lo que implica que está esparcida y necesita ser recogida primero.\n",
            "2. **`poner_ropa_en_cesto`**:  Una vez recogida, se coloca directamente en el cesto para evitar desorden adicional.\n",
            "3. **`recoger_libros`**:  Los libros en el suelo representan un segundo foco de desorden.\n",
            "4. **`colocar_libros_en_estanteria`**:  Devolvemos los libros a su lugar.\n",
            "5. **`vaciar_papelera`**:  Aunque no se menciona explícitamente, vaciar la papelera ayuda a mantener el orden general y suele ser una buena práctica al finalizar la organización.\n",
            "6. **`listo`**:  ¡Habitación organizada! \n"
          ]
        }
      ]
    }
  ]
}